# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yzvwQ6cS3rK-Nf2ekICcytejsuH04_SB
"""

from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import pandas as pd

# Placeholder definitions for missing variables
CATEG_ENCODING = "onehot"  # Can be "onehot", "label", "target"
SCALING = "standard"      # Can be "standard", "minmax", or other scaler names
NUM_COLS = []             # Replace with your actual numerical column names
CAT_COLS = []             # Replace with your actual categorical column names

# Placeholder for numerical imputer
num_imputer = SimpleImputer(strategy="mean")

# Placeholder for X DataFrame if not already defined
# X = pd.DataFrame() # Uncomment and populate if X is not defined elsewhere

from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import pandas as pd
import numpy as np

# Placeholder definitions for missing variables
CATEG_ENCODING = "onehot"  # Can be "onehot", "label", "target"
SCALING = "standard"      # Can be "standard", "minmax", or other scaler names
NUM_COLS = ['numerical_col_1', 'numerical_col_2', 'id_col']
CAT_COLS = ['categorical_col_1', 'categorical_col_2', 'mixed_col']

# Placeholder for numerical imputer
num_imputer = SimpleImputer(strategy="mean")

# Placeholder for X DataFrame if not already defined
# X = pd.DataFrame() # Uncomment and populate if X is not defined elsewhere

X_transformed = pre.fit_transform(X)
print("Shape of transformed data:", X_transformed.shape)
# Display a part of the transformed data if it's not too large (e.g., first 5 rows)
# Note: ColumnTransformer returns a sparse matrix by default for OneHotEncoder, so conversion to dense is often needed for inspection
if hasattr(X_transformed, 'toarray'):
    print("\nFirst 5 rows of transformed data (converted to dense array):")
    display(pd.DataFrame(X_transformed.toarray()).head())
else:
    print("\nFirst 5 rows of transformed data:")
    display(pd.DataFrame(X_transformed).head())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np

# Create a placeholder target variable y
# Assuming a binary classification problem for demonstration
y = np.random.randint(0, 2, size=X_transformed.shape[0])

print("Placeholder target variable y created with shape:", y.shape)
print("First 5 values of y:", y[:5])

# Split the data into training and testing sets
# Using a 80/20 split for demonstration
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42, stratify=y)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# Initialize and train a Logistic Regression model
# Set max_iter for convergence for potential sparse data
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train, y_train)

print("Logistic Regression model trained successfully.")

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate and print the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy on the test set: {accuracy:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

import pandas as pd
import numpy as np

# Create a sample DataFrame X
X = pd.DataFrame({
    'numerical_col_1': np.random.rand(100) * 100,
    'numerical_col_2': np.random.randint(1, 50, 100),
    'categorical_col_1': np.random.choice(['A', 'B', 'C'], 100),
    'categorical_col_2': np.random.choice(['X', 'Y'], 100),
    'id_col': range(100),
    'mixed_col': np.random.choice([1, 2, 'three', None], 100)
})

# Introduce some missing values for demonstration
X.loc[np.random.choice(X.index, 10), 'numerical_col_1'] = np.nan
X.loc[np.random.choice(X.index, 5), 'categorical_col_1'] = np.nan

print("Sample DataFrame X created:")
display(X.head())
print("\nDataFrame Info:")
X.info()

# Identify numerical and categorical columns automatically
NUM_COLS = X.select_dtypes(include=np.number).columns.tolist()
CAT_COLS = X.select_dtypes(include='object').columns.tolist()

print(f"Identified Numerical Columns: {NUM_COLS}")
print(f"Identified Categorical Columns: {CAT_COLS}")

from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import pandas as pd
import numpy as np

# Placeholder definitions for missing variables
CATEG_ENCODING = "onehot"  # Can be "onehot", "label", "target"
SCALING = "standard"      # Can be "standard", "minmax", or other scaler names
NUM_COLS = ['numerical_col_1', 'numerical_col_2', 'id_col']
CAT_COLS = ['categorical_col_1', 'categorical_col_2', 'mixed_col']

# Placeholder for numerical imputer
num_imputer = SimpleImputer(strategy="mean")

# Placeholder for X DataFrame if not already defined
# X = pd.DataFrame() # Uncomment and populate if X is not defined elsewhere

from sklearn.preprocessing import FunctionTransformer

# Encodage
if CATEG_ENCODING == "onehot":
    cat_encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=True)
elif CATEG_ENCODING == "label":
    # LabelEncoder ne gère pas colonnes multiples facilement; on imputera et on OneHot minimal
    cat_encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=True)
elif CATEG_ENCODING == "target":
    # Le target encoding sera géré ailleurs sur folds; ici placeholder One-Hot pour pipeline fit_transform
    cat_encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=True)
else:
    cat_encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=True)

# Scaling
if SCALING == "standard":
    scaler = StandardScaler(with_mean=False)
elif SCALING == "minmax":
    scaler = MinMaxScaler()
else:
    scaler = "passthrough"

numeric = Pipeline(steps=[
    ("imputer", num_imputer),
    ("scaler", scaler)
])

categorical = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("to_string", FunctionTransformer(lambda x: x.astype(str), validate=False)), # Ensure uniform string type
    ("encoder", cat_encoder)
])

pre = ColumnTransformer(
    transformers=[
        ("num", numeric, [c for c in NUM_COLS if c in X.columns]),
        ("cat", categorical, [c for c in CAT_COLS if c in X.columns]),
    ],
    remainder="drop",
    sparse_threshold=0.3
)

"""# Task
Define a hyperparameter grid for Logistic Regression including regularization strength (C) and solver, and perform Grid Search cross-validation to find the best combination of parameters.

## Define Hyperparameter Grid

### Subtask:
Define a grid of hyperparameters for the Logistic Regression model, including regularization strength (C) and solver, to explore different model configurations.

**Reasoning**:
I need to define a hyperparameter grid for Logistic Regression as a dictionary. This dictionary will contain different values for the regularization strength (C) and the solver to be used for the model.
"""

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'lbfgs', 'saga']
}

print("Hyperparameter grid defined:")
print(param_grid)

"""**Reasoning**:
Since the hyperparameter grid has been defined, the next step is to import GridSearchCV and other necessary libraries to perform the hyperparameter tuning.


"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

print("GridSearchCV and LogisticRegression imported.")

"""**Reasoning**:
Now that the hyperparameter grid and necessary libraries are ready, I will initialize and run GridSearchCV to find the best hyperparameters for the Logistic Regression model using cross-validation.


"""

log_reg = LogisticRegression(max_iter=1000, random_state=42)

grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

print("Starting Grid Search CV...")
grid_search.fit(X_train, y_train)
print("Grid Search CV completed.")

"""**Reasoning**:
With the Grid Search completed, the next step is to examine its results to identify the best hyperparameters and the corresponding best score.


"""

print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validation score: {:.4f}".format(grid_search.best_score_))

"""**Reasoning**:
Now that the best hyperparameters have been identified, the next step is to evaluate the performance of the model with these best parameters on the unseen test data. This will involve using the `grid_search.best_estimator_` to make predictions on `X_test` and then calculating the accuracy.


"""

best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)

accuracy_best = accuracy_score(y_test, y_pred_best)
print(f"Accuracy of the best model on the test set: {accuracy_best:.4f}")

"""## Evaluate Tuned Model

### Subtask:
Evaluate the performance of the tuned model on the test set using accuracy, confusion matrix, and classification report.

**Reasoning**:
I need to calculate the confusion matrix for the best model's predictions, then plot it and print the classification report to evaluate its performance on the test set, as instructed by the subtask.
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# Calculate the confusion matrix for the best model
cm_best = confusion_matrix(y_test, y_pred_best)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix for Tuned Model')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print("\nClassification Report for Tuned Model:")
print(classification_report(y_test, y_pred_best))

"""**Reasoning**:
The previous code produced `UndefinedMetricWarning` because there were no predicted samples for one of the classes, leading to zero division in precision calculation. To handle this explicitly and suppress the warning, I will set `zero_division=0` in the `classification_report` function.


"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# Calculate the confusion matrix for the best model
cm_best = confusion_matrix(y_test, y_pred_best)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix for Tuned Model')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print("\nClassification Report for Tuned Model:")
print(classification_report(y_test, y_pred_best, zero_division=0))

"""## Final Task

### Subtask:
Summarize the results of the hyperparameter tuning and the performance improvements of the optimized model.

## Summary:

### Q&A
The hyperparameter tuning for the Logistic Regression model identified `{'C': 0.001, 'solver': 'liblinear'}` as the best combination of parameters, achieving a best cross-validation accuracy of 0.6750. The optimized model, when evaluated on the test set, achieved an accuracy of 0.6500.

### Data Analysis Key Findings
*   A hyperparameter grid was defined for Logistic Regression, including regularization strengths (`C`: \[0.001, 0.01, 0.1, 1, 10, 100]) and solvers (\[`liblinear`, `lbfgs`, `saga`]).
*   Grid Search cross-validation found the optimal parameters to be `C = 0.001` and `solver = 'liblinear'`.
*   The best cross-validation accuracy achieved with these parameters was 0.6750.
*   The accuracy of the best model on the unseen test set was 0.6500.
*   The classification report revealed a significant class imbalance issue and model bias:
    *   For class 0 (7 samples), the model achieved 0.00 precision, recall, and f1-score, indicating it completely failed to identify any instances of this class.
    *   For class 1 (13 samples), the model performed well with a precision of 0.65, a recall of 1.00, and an f1-score of 0.79, correctly identifying all actual instances of class 1.
    *   The model exhibits a strong bias towards predicting class 1.

### Insights or Next Steps
*   The model's strong bias towards predicting class 1 and complete failure to identify class 0 suggests a severe class imbalance problem in the dataset.
*   Future steps should focus on addressing the class imbalance (e.g., using techniques like SMOTE, oversampling, undersampling, or adjusting class weights within the model) and potentially exploring other classification algorithms or feature engineering to improve the detection of class 0.
"""